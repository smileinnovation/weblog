---
layout: post
url: https://medium.com/@/9d302c723eda
title: LSTM, Intelligence artificielle sur des donnÃ©es chronologiques
subtitle: Comprendre comment sont traitÃ©es les donnÃ©es chronologiquesÂ  dans un rÃ©seau de neurones.
slug: lstm-intelligence-artificielle-sur-des-donnÃ©es-chronologiques
description: Explication du fonctionnement interne dâ€™un rÃ©seau de neurones de type LSTM ou comment la donnÃ©e est traitÃ©e dâ€™un point de vue mathÃ©matique.
tags: machine-learning,neural-networks,lstm,mathematics,data-science
author: Youcef Messaoud
username: khlemess
---

# LSTM, Intelligence artificielle sur des donnÃ©es chronologiques

### Ou comment un rÃ©seau de neurones peut interprÃ©ter une sÃ©rie de donnÃ©es dite â€œtemporelleâ€

Si vous avez un tant soit peu travaillÃ© avec des rÃ©seaux de neurones, vous avez certainement remarquÃ© que, gÃ©nÃ©ralement, nous travaillons avec des donnÃ©es â€œdâ€™entrÃ©eâ€ ponctuelle. En dâ€™autres termes, une image, un jeu de valeursâ€¦ mais il est possible de travailler avec des donnÃ©es temporelles telles que des valeurs boursiÃ¨res sur un laps de temps, des â€œframesâ€ de vidÃ©o, etcâ€¦ Pour cela, la reine des topologies se nomme LSTM. Dans cet article, nous allons tenter de vulgariser son fonctionnement afin que vous puissiez vous pencher sur le sujet plus sereinement Ã  lâ€™avenir.

![](/assets/images/posts/1*NMzhN3iG7iQgWvu3NQfMzw.jpeg)

Lâ€™intelligence artificielle est un domaine actuellement en pleine effervescence, et des dÃ©couvertes incroyables Ã©mergent chaque jour, avec de nouveaux domaines dâ€™application. Nous avons, par exemple, dÃ©jÃ  Ã©tudiÃ© les rÃ©seaux de neurones et les crypto-monnaies dans un article que nous vous conseillons de parcourir Ã©galement.

https://medium.com/smileinnovation/how-to-make-profits-in-cryptocurrency-trading-with-machine-learning-edb7ea33cee4

Mais avant dâ€™aller plus loin, il est bon de prÃ©ciser les dÃ©finitions de base afin que nous soyons tous sur la mÃªme longueur dâ€™onde pour la suite de lâ€™article.

# Le Machine Learning, câ€™est quoi ?

> Champ dâ€™Ã©tude de lâ€™intelligence artificielle, concerne la conception, lâ€™analyse, le dÃ©veloppement et lâ€™implÃ©mentation de mÃ©thodes permettant Ã  une machine (au sens large) dâ€™Ã©voluer par un processus systÃ©matique, et ainsi de prÃ©voir des rÃ©sultats par** apprentissage**, et non par la crÃ©ation dâ€™algorithmes complexes qui, potentiellement, pourraient dâ€™ailleurs fonctionner avec moins de prÃ©cision. ([Wikipedia](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjy1OaezbXdAhWTTcAKHaxABEQQFjAAegQIBBAC&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMachine_learning&usg=AOvVaw1cN2bkbECZkJm_QVt7S2RR))

Une des mÃ©thodes dâ€™apprentissage la plus connue est le *rÃ©seau de neurones*. Le rÃ©seau de neurones est une mÃ©thode de construction de modÃ¨le qui peut, par le biais de donnÃ©es dâ€™entraÃ®nement, â€œapprendreâ€ Ã  prÃ©dire des valeurs par la suite. On peut sâ€™en servir aussi bien pour analyser des donnÃ©es simples (donnÃ©es chimique, choix dâ€™utilisateur, capteurs de tempÃ©ratureâ€¦) que des donnÃ©es trÃ¨s complexes comme des images, des vidÃ©os, le cours de la bourse, etcâ€¦. Tout le monde a ce clichÃ© de lâ€™Ã©cole primaire des annÃ©es 70, oÃ¹ lorsque lâ€™Ã©lÃ¨ve rÃ©citant ses tables de multiplication se trompe et que son professeur sort sa rÃ¨gle et lui tape sur ses doigts en lui disant â€œcâ€™est faux, tu tâ€™es trompÃ© recommenceâ€. Le rÃ©seau de neurones fonctionne de la mÃªme faÃ§on, nous allons lui faire recommencer la phase dâ€™apprentissage tant que nous nâ€™avons pas le rÃ©sultat souhaitÃ©.

En termes dâ€™expÃ©riences, deux cas de figures sâ€™offrent Ã  nous :

* les expÃ©riences sont indÃ©pendantes les unes des autres

* les expÃ©riences sont sous la forme de sÃ©quences temporelles

### Le problÃ¨me

Un rÃ©seau de neurones classique permet de gÃ©rer les cas ou les expÃ©riences sont indÃ©pendantes les unes des autres. Lorsque les expÃ©riences sont sous la forme de sÃ©quences temporelles, une nouvelle structure a Ã©tÃ© inventÃ©e : les rÃ©seaux de neurones rÃ©currents. Cette nouvelle structure introduit un mÃ©canisme de mÃ©moire des entrÃ©es prÃ©cÃ©dentes qui persiste dans les Ã©tats internes du rÃ©seau et peut ainsi impacter toutes ses sorties futures. Le rÃ©seau de neurones **L**ong **S**hort **T**erm **M**emory (LSTM) est lâ€™un des plus connu.

![](/assets/images/posts/1*fMmK00klYnPN7xMl-R9Flw@2x.png)

# Un peu dâ€™histoire

Bien quâ€™il soit connu depuis 1997, lâ€™architecture de neurones **L**ong **S**hort **T**erm **M**emory connait un essor plus important depuis quelques annÃ©es, cela est dÃ» au fait que, comme pour la plupart des modÃ¨les neuronaux, nous profitons de lâ€™Ã©volution des performances de machines, mais aussi depuis que le machine learning dispose dâ€™outils mathÃ©matiques puissants pour rÃ©soudre les fameux problÃ¨mes de rÃ©tropropagation. En effet, câ€™est en 1997 que le rÃ©seau de neurones LSTM a Ã©tÃ© conceptualisÃ© [par Sepp Hochreiter et JÃ¼rgen Schmidhuber](http://www.bioinf.jku.at/publications/older/2604.pdf).

Le rÃ©seau de neurones Long Short Term Memory (LSTM) est utilisÃ© sur des donnÃ©es triÃ©es temporellement. Dans certains cas dâ€™usage, il est important de savoir quelles dÃ©cisions ont Ã©tÃ© prises dans le passÃ© afin de prendre une dÃ©cision optimale Ã  lâ€™instant *t.*

[Le LSTM a Ã©tÃ© inventÃ© pour rÃ©soudre le problÃ¨me du *vanishing and exploding gradient *rencontrÃ© dans un rÃ©seau de neurones rÃ©current classique.](http://www.bioinf.jku.at/publications/older/2304.pdf)

### Cas dâ€™utilisation

Depuis 2016, il est prÃ©sent un peu partout dans certaines fonctionnalitÃ©s et applications que vous utilisez tous les jours. Google lâ€™utilise, par exemple, pour son assistance vocale sur mobile. Apple, quant Ã  lui, lâ€™implÃ©mente dans sa fonctionnalitÃ© â€œQuickTypeâ€ qui facilite lâ€™Ã©criture sur iPhone.

Cet article nâ€™a pas pour but de refaire lâ€™enseignement du LSTM mais de proposer une vulgarisation Ã  la communautÃ© franÃ§aise.
Cependant si vos recherches vous mÃ¨nent dans des contrÃ©es plus techniques, je vous conseille la lecture de â€œUnderstanding LSTM Networksâ€ (27 aoÃ»t 2015, par Christopher Olah) ou plus rÃ©cemment avec lâ€™article â€œ[Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714) (13 Mars 2016 par [Shi Yan](https://medium.com/@shiyan?source=post_header_lockup)).

# Illustrons le principe

Pour bien comprendre le concept du rÃ©seau de neurones LSTM, nous allons imaginer une petite mise en scÃ¨ne : un club assez rÃ©putÃ© organise une soirÃ©e. Des influenceurs sont attendus. Pour faciliter lâ€™accÃ¨s, deux agents de sÃ©curitÃ© sont rÃ©quisitionnÃ©s :

* un agent de sÃ©curitÃ© pour les anciens clients.

* un agent de sÃ©curitÃ© pour les nouveaux clients.

Le premier agent sera en charge de sâ€™occuper exclusivement des anciens clients, il va dÃ©cider quels anciens clients vont Ãªtre autorisÃ©s Ã  passer lâ€™entrÃ©e. Pour ce faire, lâ€™agent de sÃ©curitÃ© dÃ©tient une liste de clients. Ceux prÃ©sents sur la liste ont lâ€™autorisation de passer la soirÃ©e dans le club. Les â€œcritÃ¨resâ€ de sÃ©lection pour apparaÃ®tre dans la liste sont :

* avoir beaucoup consommÃ© lors des prÃ©cÃ©dentes soirÃ©es.

* avoir eu un comportement irrÃ©prochable.

Les anciens clients se prÃ©sentent Ã  la porte dâ€™entrÃ©e et exposent leur carte dâ€™identitÃ© Ã  lâ€™agent de sÃ©curitÃ©, il vÃ©rifie sa liste et refuse ou accepte lâ€™accÃ¨s au club des anciens. En fin de soirÃ©e, lâ€™agent de sÃ©curitÃ© met Ã  jour sa liste dâ€™anciens clients qui servira aux prochaines soirÃ©es.

![](/assets/images/posts/1*mGMD3FquVQ-H4of-pfmPIQ@2x.png)

Le second agent de sÃ©curitÃ© est moins sÃ©lectif Ã  lâ€™entrÃ©e, mais il va Ãªtre bien plus strict Ã  lâ€™intÃ©rieur. Les nouveaux clients se prÃ©sentent Ã  lâ€™entrÃ©e, lâ€™agent de sÃ©curitÃ© laisse passer tout le monde peu importe leur tenue, ici le mot dâ€™ordre est *â€œ laissons-leur une chanceâ€œ*. Une fois que tous les nouveaux clients sont Ã  lâ€™intÃ©rieur du club, lâ€™agent de sÃ©curitÃ© dÃ©cide, Ã  son tour, de rentrer dans le club. Lors de la soirÃ©e, tout les clients sont scrutÃ©s et leur comportement scrupuleusement Ã©valuÃ© par lâ€™agent de sÃ©curitÃ©. La notoriÃ©tÃ© du club est en jeu, les clients qui vont Ãªtre agressifs, Ã©mÃ©chÃ©s, ou peu respectueux vont Ãªtre rappelÃ©s Ã  lâ€™ordre par lâ€™agent de sÃ©curitÃ©. Les nouveaux clients qui consomment et qui contribuent Ã  une bonne ambiance vont Ãªtre notÃ©s sur une liste. Les clients prÃ©sents sur cette liste auront le privilÃ¨ge dâ€™Ãªtre considÃ©rÃ©s comme bons clients.

![](/assets/images/posts/1*kHK5TgPFGgirSCaxpBPUxw@2x.png)

En fin de soirÃ©e, les deux agents de sÃ©curitÃ© se rÃ©unissent afin de fusionner les deux listes pour nâ€™en former quâ€™une seule qui fera office de nouvelle liste pour les prochaines soirÃ©es.

![La liste Ã  Ã©tÃ© mise Ã  jour](/assets/images/posts/1*0LxVjGduh99tkqPcwC5mYg@2x.png)

### Club = LSTM ? (OÃ¹ veulent-ils en venir ? )

La gestion de la mÃ©moire dâ€™un bloc LSTM Ã  lâ€™autre fonctionne de la mÃªme maniÃ¨re. ConcrÃ¨tement, un bloc LSTM comporte 3 portes : une porte dâ€™oubli qui jouera le rÃ´le dâ€™agent de sÃ©curitÃ© en charge des anciens clients, une porte dâ€™entrÃ©e qui jouera le rÃ´le dâ€™agent de sÃ©curitÃ© en charge des nouveaux clients ainsi quâ€™une porte de sortie que nous expliquerons dans les parties suivantes.

# Struture du rÃ©seaux LSTM (Long Short Term Memory)

Une couche LSTM est donc plus ou moins illustrable de cette maniÃ¨re :

![](/assets/images/posts/1*zL2bxCq32YCPiXoTZv1MJQ.png)

### Porte dâ€™oubli

![Porte dâ€™oubli](/assets/images/posts/1*-batepYVu8MlxZEpyEemvg.png)

La porte dâ€™oubli va se charger de filtrer les informations contenues dans la cellule mÃ©moire prÃ©cÃ©dente. Parmis ces informations, certaines vont Ãªtre plus pertinentes que dâ€™autre. Pour dÃ©cider quelles valeurs vont Ãªtre autorisÃ©es Ã  passer, nous allons utiliser diffÃ©rents calculs mathÃ©matiques :

![](/assets/images/posts/1*s0Whizedv39uY274AwWgvA@2x.jpeg)

*xt* correspond aux donnÃ©es entrantes dans le rÃ©seau de neurones, *ht-1 *correspond Ã  la valeur de sortie qui a Ã©tÃ© prÃ©dite par la couche LSTM prÃ©cedente. Dâ€™un point de vue mathÃ©matique, *[ht-1,xt]* dÃ©signe la concatÃ©nation des tableaux *xt *et *ht-1. *Comme dans un rÃ©seau de neurones classique, Wf correspond au poids des neurones et bf correspond au biais (le biais est considÃ©rÃ© comme un degrÃ© de libertÃ© supplÃ©mentaire) dâ€™un rÃ©seau de neurones. Tout ce beau monde est passÃ© comme paramÃ¨tre Ã  une fonction de transfert nommÃ©e sigmoÃ¯de. En sortie, la fonction sigmoÃ¯de nous renvoie un entier compris entre 0 et 1.

![formule mathÃ©matique de la fonction sigmoÃ¯de](/assets/images/posts/1*BLvRJO6olfp59A5n7NViSQ@2x.png)

![courbe reprÃ©sentative de la fonction sigmoÃ¯de](/assets/images/posts/1*wXKWMlxnfDUhH1be197lVA@2x.png)

Si vous vous demandez pourquoi une fonction sigmoÃ¯de est utilisÃ©e, la rÃ©ponse est : juste parce quâ€™elle est sympa et par simple hasardâ€¦ trÃªve de plaisanteries ! En vÃ©ritÃ© câ€™est bien plus compliquÃ©. Je vais vous donner quelques pistes et Ã  vos claviers pour les recherches : Elle reprÃ©sente la fonction de rÃ©partition de la loi logistique et est souvent utilisÃ©e dans les rÃ©seaux de neurones parce quâ€™elle est dÃ©rivable en tout point, ce qui est une contrainte pour lâ€™algorithme de rÃ©tro-propagation qui a permis de crÃ©er des rÃ©seaux multicouches. Et enfin, chose intÃ©ressante, cette fonction a une tendance Ã  lisser les valeurs de sortie.

*ft* est donc un tableau de valeurs comprises entre 0 et 1. Pour savoir quelles valeurs de la cellule mÃ©moire prÃ©cÃ©dente *Ct-1* vont Ãªtre autorisÃ©es Ã  passer, nous allons appliquer la multiplication terme Ã  terme entre* ft* et *Ct-1*. Si, lors de cette multiplication la valeur de sortie est proche de 1 alors la valeur en question (valeur contenue dans Ct-1) est autorisÃ©e Ã  passer. Dans le cas contraire, lâ€™information contenue dans Ct-1 est ignorÃ©e si la multiplication donne une valeur proche de 0. Toutes les valeurs qui ont rÃ©ussis Ã  passer sont stockÃ©es dans C1.

![mise Ã  jour de la cellule mÃ©moire](/assets/images/posts/1*tfztpeN2yhyxOTTurA7EWQ@2x.png)

![agent de sÃ©curitÃ© en charge des anciens clients](/assets/images/posts/1*ZSUo0qVNfzdOO4VSy17XWA.png)

**Avez-vous compris ?**

La porte dâ€™oubli se charge uniquement de filtrer les anciennes valeurs contenues dans la cellule mÃ©moire prÃ©cÃ©dente.

### Porte dâ€™entrÃ©e

AprÃ¨s avoir mis Ã  jour les valeurs prÃ©sentes dans la cellule mÃ©moire prÃ©cÃ©dente, il est temps de dÃ©cider quelles nouvelles informations (nouveaux clients) vont Ãªtre autorisÃ©es Ã  Ãªtre stockÃ©es dans la cellule mÃ©moire.

![porte dâ€™entrÃ©e](/assets/images/posts/1*wkBEvbAdps9n91cuAZ6j-w.png)

La porte dâ€™entrÃ©e va se charger de dÃ©cider quelles nouvelles valeurs provenant de *xt *(valeurs entrantes du LSTM) vont Ãªtre autorisÃ©es Ã  passer.

Certaines entrÃ©es vont Ãªtre plus pertinentes que dâ€™autres. Celles qui sont pertinentes vont Ãªtre stockÃ©es dans la cellule mÃ©moire qui va servir Ã  la prise de dÃ©cision Ã  lâ€™instant t et peut-Ãªtre pour lâ€™instant *t+1 *si la porte dâ€™oubli lui autorise lâ€™accÃ¨s. Comme pour la porte dâ€™oubli, diffÃ©rentes formules mathÃ©matiques vont Ãªtre nÃ©cessaires pour savoir quelles informations vont Ãªtre autorisÃ©es Ã  passer :

![](/assets/images/posts/1*YFZpCWwMxm_qCE-r3L9kgg@2x.png)

Dans un premier temps, une fonction sigmoÃ¯de est nÃ©cessaire pour dÃ©cider quelles nouvelles entrÃ©es seront susceptibles dâ€™Ãªtre stockÃ©es dans la cellule mÃ©moire. *it*** **sera un tableau et chacune de ses valeurs sera comprise entre 0 et 1. Dans un second temps, nous allons faire la mÃªme chose sauf que la fonction dâ€™activation utilisÃ©e est la fonction tangente hyperbolique (câ€™est une sigmoÃ¯de centrÃ© entre -1 et 1).

![formule mathÃ©matique de la tangente hyperbolique](/assets/images/posts/1*L0BULNKuf86dTjg9Tq6Wnw@2x.png)

![Courbe reprÃ©sentative de la tangente hyperbolique](/assets/images/posts/1*9d3Vdv1NANKd3XtV0dqL0Q@2x.png)

*C~* est un tableau dont chaque valeur sera comprise entre -1 et 1. *C~* contient les valeurs candidates Ã  Ãªtre stockÃ©es dans la cellule mÃ©moire. La multiplication terme Ã  terme entre *it* et *C~* nous permettra de savoir quelles valeurs vont devoir Ãªtre stockÃ©es dans la cellule mÃ©moire. Si le rÃ©sultat est proche de 1 alors nous allons stocker la valeur correspondante dans la cellule mÃ©moire, au contraire si le rÃ©sultat est proche de 0, lâ€™information sera bloquÃ©e. La nouvelle mÃ©moire sera donc :

![mÃ©moire mise Ã  jour](/assets/images/posts/1*wm9LF6vMMe2ECA1M2FRoCQ@2x.png)

![agent de sÃ©curitÃ© en charge des nouveaux clients](/assets/images/posts/1*wLq9Q18rIgatF6uNCpK0tg.png)

**En bref**

La porte dâ€™entrÃ©e va, parmi les nouvelles entrÃ©es, laisser passer les informations qui sont pertinentes (nouveaux clients). Ces valeurs-lÃ  seront stockÃ©es dans la cellule mÃ©moire et auront une importance dans la prise de dÃ©cision Ã  lâ€™instant *t* et peut-Ãªtre pour lâ€™instant *t+1*.

### Porte de sortie

![porte de sortie](/assets/images/posts/1*ZSOd-zBJhnbmksiasiFv_g.png)

Il est temps, en fonction de lâ€™entrÃ©e (*xt*), de fournir une dÃ©cision *ht* Ã  la fois pour le bloc LSTM suivant ainsi que pour lâ€™utilisateur. La prise de dÃ©cision *ht *est calculÃ©e de la sorte :

![](/assets/images/posts/1*hOuuwRCFGd3QnF6uvH4T4w@2x.png)

La sortie* ht* va dÃ©pendre des informations prÃ©sentes dans la cellule mÃ©moire *Ct *ainsi que des informations entrantes *xt* dans la couche LSTM.

### Long Short Time Memory

Dans un soucis de chronologie, ils est important dâ€™assembler plusieurs bloc LSTM afin de crÃ©er une sÃ©rie temporelle qui servira Ã  la prÃ©diction dâ€™une action.

![](/assets/images/posts/1*qfPsBPv9zo8gBAPav_hb5w@2x.png)

# Conclusion

Le rÃ©seau de neurones LSTM est utilisÃ© dans plusieurs domaines. En effet, savoir ce qui sâ€™est passÃ© prÃ©cÃ©demment afin de prÃ©dire une action Ã  lâ€™instant *t* est trÃ¨s important.

Afin de dÃ©tecter et donc de prÃ©dire les pannes de leurs machines, les industriels vont Ã©tudier chronologiquement chaque paramÃ¨tres de leurs machines (tempÃ©rature de la machine, date de la derniÃ¨re rÃ©paration de la machine, nombre de piÃ¨ces fabriquÃ©esâ€¦). Amazon lâ€™utilise Ã©galement dans sa reconnaissance vocale utilisÃ©e par Alexa, et oui grÃ¢ce au LSTM la voix dâ€™Alexa contrairement Ã  ce que lâ€™on peut croire nâ€™est pas un enregistrement de voix fÃ©minine humaine. le LSTM peut bien entendu Ãªtre utilisÃ© pour reconnaÃ®tre un mouvement (en combinaison avec des couches convolutionnelles) puisque nous devons nous baser sur des â€œframesâ€ dans un temps donnÃ©, ou encore pour Ã©tudier le cours de la bourse comme nous lâ€™avons fait dans lâ€™article dont le lien se trouve en haut de cette page.

En bref, LSTM est une topologie neuronale extrÃªmement utile Ã  partir du moment oÃ¹ une â€œsÃ©rie temporelleâ€ de donnÃ©es est en jeu.

Lâ€™explication dâ€™une telle architecture nâ€™est pas Ã©vidente, câ€™est pour cela que nous avons introduit un exemple comprÃ©hensif. Si des articles techniques tel que celui-ci vous intÃ©ressent, nâ€™hÃ©sitez pas Ã  nous proposer dâ€™autres pistes Ã  explorer.

Avez-vous apprÃ©ciÃ© lâ€™article ? Si oui, nâ€™hÃ©sitez pas Ã  ğŸ‘ notre article ou abonnez-vous Ã  notre newsletter Innovation watch!
Vous pouvez suivre Smile sur F[acebook,](https://www.facebook.com/smileopensource) T[witter ](https://www.twitter.com/GroupeSmile)et Y[outube.](http://www.youtube.com/user/SmileOpenSource)

Co-auteur : [Quentin Le Baron](), [Youcef Messaoud]()

RÃ©fÃ©rences

* [Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714) (by [Shi Yan]() )

* [Understanding LSTM Networks (by Christopher Olah blog)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

* [LSTM original publication (by Sepp Hochreiter and JÃ¼rgen Schmidhuber)](http://www.bioinf.jku.at/publications/older/2604.pdf)

* [A Beginnerâ€™s Guide to LSTMs (skymind)](https://skymind.ai/wiki/lstm)

* Character designed by i[conicbestiary](https://fr.freepik.com/iconicbestiary) /[ Freepik](https://fr.freepik.com/)

* [Reseaux de neurones recurrents pour le traitement automatique de la parole](https://tel.archives-ouvertes.fr/tel-01615475/document)


